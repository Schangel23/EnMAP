{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EO-Lab Tutorial: Query and Download of EnMAP Data from the EOC Geoservice\n",
    "\n",
    "This notebook demonstrates how to query and download EnMAP data from the EOC Geoservice STAC catalogue using a curl-based approach. The notebook is divided into several cells for clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Cookie Instructions\n",
    "\n",
    "To download data from the EOC Geoservice using `curl`, you must first obtain the session cookie from the EOC Geoservice UMS. Follow these steps:\n",
    "\n",
    "1. **Access the Login Page**: Open the following link in your browser:\n",
    "   \n",
    "   [EO-LAB (ENMAP) Login](https://sso.eoc.dlr.de/eoc/auth/login?service=https://download.geoservice.dlr.de/ENMAP/files/L2A/)\n",
    "   \n",
    "   On the right side under **External Identity Providers**, click **EO-LAB (ENMAP)**.\n",
    "\n",
    "2. **Log in**: Enter your EO-Lab credentials when prompted.\n",
    "\n",
    "3. **Open Developer Tools**: Press **F12** (or open your browser's Web Developer Tools). If you miss it, refresh the page with **F5**.\n",
    "\n",
    "4. **Locate the GET Request**: In the **Network** tab, look for a GET request resembling the following URL:\n",
    "   \n",
    "   `https://download.geoservice.dlr.de/ENMAP/files/L2A/?ticket=ST-11848-nWxfdPAbhjmUoQdgPeS8uTr-gps-auth`\n",
    "\n",
    "5. **Copy the cURL Command**: Right-click on that request and select **Copy as cURL**. The exact command you copy does not matter since the code handles both POSIX and Windows environments.\n",
    "\n",
    "6. **Update the Notebook**: Replace the example `CURL_COMMAND` in the user configuration cell with your own cURL command containing your session cookie.\n",
    "\n",
    "Note: Session cookies expire over time. For long downloads or additional files, repeat this procedure to obtain a new session cookie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "The following cell imports all the necessary libraries required by the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os              # For operating system interactions (file paths, directories)\n",
    "import re              # For regular expressions (pattern matching and extraction)\n",
    "import requests        # For making HTTP requests (downloading files)\n",
    "from datetime import datetime  # For date/time manipulation if needed\n",
    "from pystac_client import Client  # For accessing and querying the STAC catalogue\n",
    "from pathlib import Path  # For high-level file system path operations\n",
    "from math import cos, radians  # For geographic calculations (used to compute bounding boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Configuration\n",
    "\n",
    "The following cell contains user-configurable parameters. Update the `CURL_COMMAND` with your session cookie, set filtering options, define which collections and assets to download, and specify search parameters (area of interest, time range, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "#      USER CONFIGURATION\n",
    "# ---------------------------\n",
    "\n",
    "# CURL command simulating a browser request; used to extract necessary HTTP headers.\n",
    "# IMPORTANT: Replace this example with your own cURL command (obtained as explained above).\n",
    "# You can paste your cURL command directly as a single line within the triple quotes.\n",
    "# For example:\n",
    "# CURL_COMMAND = \"\"\"curl \"https://download.geoservice.dlr.de/ENMAP/files/L2A/?ticket=YOUR_TICKET\" -H \"User-Agent: YourAgent\" -H \"Cookie: session=YOUR_SESSION\" \"\"\" \n",
    "CURL_COMMAND = \"\"\"\n",
    "curl \"https://download.geoservice.dlr.de/ENMAP/files/L2A/?ticket=ST-11848-nWxfdPAbhjmUoQdgPeS8uTr-gps-auth\" \\\n",
    "  -H \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0\" \\\n",
    "  -H \"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8\" \\\n",
    "  -H \"Accept-Language: de-DE,de;q=0.8,en-US;q=0.5,en;q=0.3\" \\\n",
    "  -H \"Accept-Encoding: gzip, deflate, br, zstd\" \\\n",
    "  -H \"DNT: 1\" \\\n",
    "  -H \"Connection: keep-alive\" \\\n",
    "  -H \"Cookie: session=Gpmbm3K-UAvKO0UmTO49OQ^|1739786884^|9zxCUa2Pz8OLhoe4zRQSYX515svNoPyKsLa93-7PevePOm7SkQBhBtKkWtOczQDe_v7f36KBsDIvQ9JIeJynkJntLlgpB3RKfsyqZV7fblipH2uMRJE3n4BDurIwMymM9KL4I8dEOruktKRpLCdgxyjurRXAQH80bK97bXq5Y2k^|cPfahrZ_q1kpCsKub_5j4QiiPlI\" \\\n",
    "  -H \"Upgrade-Insecure-Requests: 1\" \\\n",
    "  -H \"Sec-Fetch-Dest: document\" \\\n",
    "  -H \"Sec-Fetch-Mode: navigate\" \\\n",
    "  -H \"Sec-Fetch-Site: same-site\" \\\n",
    "  -H \"Priority: u=0, i\"\n",
    "\"\"\"\n",
    "\n",
    "# Extract base URL from CURL_COMMAND (for potential modifications)\n",
    "CURL_PARTS = CURL_COMMAND.split(' ')\n",
    "BASE_URL = CURL_PARTS[1][1:-1]\n",
    "\n",
    "# Cloud cover filter: only process items with cloud cover below the specified maximum.\n",
    "CLOUD_COVER_FILTER = {\n",
    "    \"enabled\": True,         # Set to True to enable filtering.\n",
    "    \"max_coverage\": 5.0        # Maximum allowed cloud cover percentage.\n",
    "}\n",
    "\n",
    "# Flag to print all properties (metadata) for each item.\n",
    "PRINT_PROPERTIES = True\n",
    "\n",
    "# Define downloads for each collection.\n",
    "\n",
    "# L2A Collection - uses lowercase asset names\n",
    "DOWNLOADS = {\n",
    "    \"ENMAP_HSI_L2A\": {\n",
    "        \"enabled\": True,  # Set to True to download this collection\n",
    "        \"assets\": [\n",
    "            \"image\",                # Main spectral image\n",
    "            #\"metadata\",           # Metadata file\n",
    "            #\"vnir\",              # VNIR sensor data\n",
    "            #\"swir\",              # SWIR sensor data\n",
    "            #\"thumbnail\",         # Small preview image\n",
    "            #\"quality_classes\",   # Quality classification\n",
    "            #\"quality_cloud\",     # Cloud mask\n",
    "            #\"quality_cloud_shadow\", # Cloud shadow mask\n",
    "            #\"quality_haze\",      # Haze mask\n",
    "            #\"quality_cirrus\",    # Cirrus mask\n",
    "            #\"quality_snow\",      # Snow mask\n",
    "            #\"quality_testflags\", # Quality test flags\n",
    "            #\"defective_pixel_mask\" # Mask of defective pixels\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # L0 Quicklook Collection - uses UPPERCASE asset names\n",
    "    \"ENMAP_HSI_L0_QL\": {\n",
    "        \"enabled\": False,  # Set to True to download this collection\n",
    "        \"assets\": [\n",
    "            \"THUMBNAIL\",          # Small preview image\n",
    "            \"OVERVIEW\",           # Larger preview image\n",
    "            #\"VNIR\",             # VNIR sensor quicklook\n",
    "            #\"SWIR\",             # SWIR sensor quicklook\n",
    "            #\"QUALITY_CLOUD\",    # Cloud mask\n",
    "            #\"QUALITY_CLOUDSHADOW\", # Cloud shadow mask\n",
    "            #\"QUALITY_CIRRUS\",   # Cirrus mask\n",
    "            #\"QUALITY_CLASSES\",  # Quality classification\n",
    "            #\"QUALITY_SNOW\",     # Snow mask\n",
    "            #\"QUALITY_HAZE\",     # Haze mask\n",
    "            #\"PIXELMASK_VNIR\",   # VNIR pixel mask\n",
    "            #\"PIXELMASK_SWIR\",   # SWIR pixel mask\n",
    "            #\"TESTFLAGS_SWIR\",   # SWIR test flags\n",
    "            #\"TESTFLAGS_VNIR\"    # VNIR test flags\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Area of interest configuration:\n",
    "# Option 1: Define a bounding box [west, south, east, north]\n",
    "BBOX = [11.230259, 48.051808, 11.337891, 48.117059]  # Example: DLR Oberpfaffenhofen\n",
    "\n",
    "# Option 2: Use a center coordinate with a specified box size (km)\n",
    "USE_CENTER_COORD = True  # Set to True to use center coordinate instead of a bounding box.\n",
    "CENTER_COORD = {\n",
    "    \"lat\": 50.71868778231684,  # Center latitude.\n",
    "    \"lon\": 7.158329088235492,  # Center longitude.\n",
    "    \"size_km\": 3              # Size of the square area in kilometers.\n",
    "}\n",
    "\n",
    "# Time range for the search (format: YYYY-MM-DD)\n",
    "START_DATE = \"2024-01-01\"  # Start date.\n",
    "END_DATE = \"2024-12-31\"    # End date.\n",
    "\n",
    "# Maximum number of items to process per collection (None means all found items)\n",
    "MAX_ITEMS = None\n",
    "\n",
    "# Download directory settings:\n",
    "CUSTOM_DOWNLOAD_PATH = None  # e.g., \"/home/user/my_enmap_data\" or \"C:/EnMAP_Data\"\n",
    "BASE_DIR = \"EnMAP_downloads\"  # Default directory if no custom path is provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "The following cell defines helper functions used to process the cURL command and calculate the bounding box from a center coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_curl_command(curl_command):\n",
    "    # Clean and format the provided CURL command.\n",
    "    # - Removes extra newlines and spaces.\n",
    "    # - Fixes cookie formatting (replaces '^|' with '|').\n",
    "    # - Ensures the command is properly quoted.\n",
    "    curl_command = ' '.join(curl_command.split())\n",
    "    if 'Cookie:' in curl_command:\n",
    "        cookie_start = curl_command.find('Cookie:')\n",
    "        cookie_end = curl_command.find('\" -H', cookie_start)\n",
    "        if cookie_end == -1:\n",
    "            cookie_end = curl_command.find('\"', cookie_start + 15)\n",
    "        cookie_part = curl_command[cookie_start:cookie_end]\n",
    "        cleaned_cookie = cookie_part.replace('^|', '|')\n",
    "        curl_command = curl_command[:cookie_start] + cleaned_cookie + curl_command[cookie_end:]\n",
    "    if not curl_command.startswith('\"\"\"'):\n",
    "        curl_command = '\"\"\"' + curl_command + '\"\"\"'\n",
    "    return curl_command\n",
    "\n",
    "def parse_curl_command(curl_command):\n",
    "    # Extract headers from the provided CURL command.\n",
    "    # Returns a dictionary mapping header names to their values.\n",
    "    curl_command = clean_curl_command(curl_command)\n",
    "    headers = {}\n",
    "    header_pattern = r'-H\\s*\"([^:]+):\\s*([^\\\"]+)\"'\n",
    "    matches = re.findall(header_pattern, curl_command)\n",
    "    for header, value in matches:\n",
    "        headers[header] = value\n",
    "    return headers\n",
    "\n",
    "def create_bbox_from_center(lat, lon, size_km):\n",
    "    # Create a bounding box from a center coordinate and box size.\n",
    "    # Returns a list [west, south, east, north] representing the bounding box.\n",
    "    km_per_degree_lat = 111.0\n",
    "    km_per_degree_lon = 111.0 * cos(radians(lat))\n",
    "    lat_offset = (size_km / 2) / km_per_degree_lat\n",
    "    lon_offset = (size_km / 2) / km_per_degree_lon\n",
    "    west = lon - lon_offset\n",
    "    east = lon + lon_offset\n",
    "    south = lat - lat_offset\n",
    "    north = lat + lat_offset\n",
    "    return [west, south, east, north]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EnmapDownloader Class\n",
    "\n",
    "The following cell defines the `EnmapDownloader` class, which searches the STAC catalogue, filters items (e.g., by cloud cover), and downloads the specified asset types. It also optionally prints item properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnmapDownloader:\n",
    "    def __init__(self):\n",
    "        self.catalog = Client.open(\"https://geoservice.dlr.de/eoc/ogc/stac/v1/\")\n",
    "        self.headers = parse_curl_command(CURL_COMMAND)\n",
    "        \n",
    "    def setup_download_directory(self, collection_name):\n",
    "        if CUSTOM_DOWNLOAD_PATH:\n",
    "            base_path = os.path.expanduser(CUSTOM_DOWNLOAD_PATH)\n",
    "            target_directory = os.path.join(base_path, collection_name)\n",
    "        else:\n",
    "            target_directory = os.path.join(BASE_DIR, collection_name)\n",
    "        try:\n",
    "            if not os.path.exists(target_directory):\n",
    "                os.makedirs(target_directory)\n",
    "                print(f\"\\nCreated directory at: {target_directory}\")\n",
    "            else:\n",
    "                print(f\"\\nUsing existing directory at: {target_directory}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating directory: {str(e)}\")\n",
    "            print(\"Falling back to default directory\")\n",
    "            target_directory = os.path.join(BASE_DIR, collection_name)\n",
    "            if not os.path.exists(target_directory):\n",
    "                os.makedirs(target_directory)\n",
    "        return target_directory\n",
    "\n",
    "    def download_file(self, url, output_path):\n",
    "        if 'Referer' in self.headers:\n",
    "            self.headers['Referer'] = os.path.dirname(url) + '/'\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, stream=True, allow_redirects=True)\n",
    "            response.raise_for_status()\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            with open(output_path, 'wb') as f:\n",
    "                if total_size == 0:\n",
    "                    f.write(response.content)\n",
    "                else:\n",
    "                    downloaded = 0\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                            downloaded += len(chunk)\n",
    "                            progress = int(50 * downloaded / total_size)\n",
    "                            print(f\"\\rProgress: [{'=' * progress}{' ' * (50 - progress)}] {downloaded}/{total_size} bytes\", end='')\n",
    "            print(f\"\\nSuccessfully downloaded: {output_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading file: {str(e)}\")\n",
    "            if os.path.exists(output_path):\n",
    "                os.remove(output_path)\n",
    "            return False\n",
    "\n",
    "    def search_and_download_collection(self, collection, asset_types):\n",
    "        print(f\"\\nSearching for {collection} data...\")\n",
    "        search_params = {\n",
    "            \"collections\": [collection],\n",
    "            \"bbox\": create_bbox_from_center(CENTER_COORD[\"lat\"], CENTER_COORD[\"lon\"], CENTER_COORD[\"size_km\"]) if USE_CENTER_COORD else BBOX\n",
    "        }\n",
    "        if START_DATE and END_DATE:\n",
    "            search_params[\"datetime\"] = f\"{START_DATE}T00:00:00Z/{END_DATE}T23:59:59Z\"\n",
    "        try:\n",
    "            search = self.catalog.search(**search_params)\n",
    "            total_matches = search.matched()\n",
    "            if total_matches == 0:\n",
    "                print(\"No items found matching your criteria.\")\n",
    "                return\n",
    "            print(f\"Found {total_matches} items.\")\n",
    "            items = list(search.items())\n",
    "            if CLOUD_COVER_FILTER[\"enabled\"]:\n",
    "                filtered_items = []\n",
    "                print(\"\\nFiltering by cloud cover...\")\n",
    "                for item in items:\n",
    "                    cloud_cover = float(item.properties.get(\"eo:cloud_cover\", 100.0))\n",
    "                    if cloud_cover <= CLOUD_COVER_FILTER[\"max_coverage\"]:\n",
    "                        filtered_items.append(item)\n",
    "                items = filtered_items\n",
    "                print(f\"After cloud cover filtering ({CLOUD_COVER_FILTER['max_coverage']}% max): {len(items)} items\")\n",
    "            items_to_process = min(MAX_ITEMS, len(items)) if MAX_ITEMS else len(items)\n",
    "            print(f\"Will download {items_to_process} items.\")\n",
    "            download_dir = self.setup_download_directory(collection.split('_')[-1])\n",
    "            items_processed = 0\n",
    "            for item in items:\n",
    "                if MAX_ITEMS and items_processed >= MAX_ITEMS:\n",
    "                    break\n",
    "                print(f\"\\nProcessing item {items_processed + 1}/{items_to_process}\")\n",
    "                print(f\"Cloud cover: {item.properties.get('eo:cloud_cover', 'N/A')}%\")\n",
    "                if PRINT_PROPERTIES:\n",
    "                    print(\"Item properties:\")\n",
    "                    for key, value in item.properties.items():\n",
    "                        print(f\"  {key}: {value}\")\n",
    "                assets = item.get_assets()\n",
    "                if items_processed == 0:\n",
    "                    print(\"\\nAvailable assets in first item:\")\n",
    "                    for asset_name in assets.keys():\n",
    "                        print(f\"- {asset_name}\")\n",
    "                    print()\n",
    "                for asset_type in asset_types:\n",
    "                    if asset_type in assets:\n",
    "                        asset = assets[asset_type]\n",
    "                        filename = os.path.basename(asset.href)\n",
    "                        output_path = os.path.join(download_dir, filename)\n",
    "                        print(f\"\\nDownloading {asset_type}: {filename}\")\n",
    "                        self.download_file(asset.href, output_path)\n",
    "                    else:\n",
    "                        print(f\"\\nAsset type {asset_type} not found in item\")\n",
    "                items_processed += 1\n",
    "            print(f\"\\nDownload complete for {collection}!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during search and download: {str(e)}\")\n",
    "\n",
    "    def download_all(self):\n",
    "        for collection, config in DOWNLOADS.items():\n",
    "            if config[\"enabled\"]:\n",
    "                print(f\"\\n{'=' * 50}\")\n",
    "                print(f\"Processing collection: {collection}\")\n",
    "                print(f\"{'=' * 50}\")\n",
    "                self.search_and_download_collection(collection, config[\"assets\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Script Execution\n",
    "\n",
    "The final cell initiates the download process by creating an instance of `EnmapDownloader` and calling its `download_all()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    downloader = EnmapDownloader()\n",
    "    downloader.download_all()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
